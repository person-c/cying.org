---
title: Machine Learning Math
date: '2022-07-18'
slug: machine-learning-math
---

## Liner algebra

## 引子

原文链接: https://www.zhihu.com/question/311724817/answer/695067704

线性代数的主线就是线性空间以及线性映射，整个线性代数的概念公式定义定理都是围绕着线性空间以及线性映射展开的。你要做的，就是紧紧抓住这条主线，把线性代数的所有知识点串联起来，然后融会贯通，自然就能学好线性代数了

#### 线性空间

线性空间的定义比较抽象，简单的说，就是向量组成的一个集合，这个集合可以定义加法以及纯量乘法，并且对加法以及乘法满足交换律结合律以及分配律。这个集合以及定义在集合上的代数运算就是线性空间。

研究线性空间有几个途径，一是基与维数，二是同构，三是子空间与直和以及商空间，四是线性映射。

先讲讲基与维数。一个线性空间必定存在基，线性空间的任意元素都可以由基线性表出，且表出方式唯一，这个唯一的表出的组合就是这个元素在这个基下的坐标。线性表出且表出方式唯一的充分必要条件是什么？这里又引出了线性无关以及极大线性无关组的概念，极大线性无关组元素的个数又能引出秩的概念。由秩又能引出维度的概念。以上这些概念都是为了刻画线性空间的基与维数而衍生出来的，并不是凭空出现无中生有的。

下面再谈谈同构。线性空间千千万，应如何研究呢？同构就是这样一个强大的概念，任何维数相同的线性空间之间是同构的，空间的维数是简单而深刻的，简单的自然数居然能够刻画空间最本质的性质。借助于同构，要研究任意一个n维线性空间，只要研究Rⁿ就行了。

n维线性空间作为一个整体，我们自然想到能不能先研究它的局部性质？所以自然而然的导出了子空间的概念以及整个空间的直和分解。直和分解要求把整个空间分解为两两不交的子空间之和，通过研究各个简单的子空间的性质，从而得出整个空间的性质。

#### 线性映射

前面讲了线性空间，舞台搭好了，轮到主角：线性映射登场了。线性映射的定义这里就不赘述了。我们小学就学过正比例函数y=kx，这是一个最简单的一维线性映射，也是一个具体的线性映射'模型'，线性映射的所有性质对比着正比例函数来看，一切都是那么简单易懂。现在把定义域从一维升级到多维，值域也从一维升级到多维，然后正比例系数k也升级为一个矩阵，那么这个正比例函数就升级为一个线性映射了。

1. 线性映射的核空间 
 
这是线性映射的一个重要的概念，什么是线性映射的核空间呢？简单的说，就是映射到零的原像的集合，记作KER。用正比例函数来类比，显然当k不等于0时，它的核是零空间，当k为零时，它的核空间是整个R。有时候需要判定一个线性映射是不是单射，按照定义来还是没那么好证的，这时我们可以从它的核来判定，只要它的核是零，那么这个线性映射必然是单射。

2. 线性映射的像 

当自变量取遍整个定义域时，它的像的取值范围成为一个线性子空间，称为像空间，记作IM。

3. 线性映射的矩阵表示  
一个抽象的线性映射应如何'解析'的表达出来呢？这个表达式写出来就是一个矩阵，且这个矩阵依赖于基的选择。也就是说在不同的基下，线性映射有不同的矩阵。基有无穷个，相应的矩阵有无穷个。这就给用矩阵研究线性映射带来了麻烦。

幸好我们有相似矩阵。同一个线性映射在不同的基下的矩阵是相似关系，相似不变量有秩，行列式，迹，特征值，特征多项式等。

所以可以通过相似矩阵来研究线性映射的秩，行列式，迹，特征值，特征多项式等性质。线性映射的矩阵有无穷多，那么这其中有哪些是值得关注的呢？第一就是标准正交基下的矩阵了，这也是最常见的。

然而一个线性映射的矩阵在标准正交基下可能特别复杂，所以需要选择一组特殊的基，让它的矩阵在这个基下有最简单的矩阵表示。如果存在这样的基，使得线性映射的矩阵为对角矩阵，则称这个线性映射可对角化。

然而是不是所有线性映射都可以对角化呢，遗憾的是，并不是。那么就要问，如果一个线性映射不能对角化，那么它的最简矩阵是什么？这个问题的答案是若尔当标准型。可以证明，在复数域上，任何线性映射都存在唯一的若尔当标准型。


向量，矩阵，符号记法，按需求理解。

vector addition rule, scalar multiplication
vector: size, angle, projection

size: square root of its sum of  component square  
projection: dot projection = dot projection  
dot product: 
- r vector's projection onto s vector 
- it's value equal r vector's projection size onto s vector
- equal |s||r|cos $\theta$ 
- scalar projection equal it's projection size divided by |s|
- projection vector equal it's scalar projection multiply s vector's united vector
- r's own dot product's square root equal its size.


矩阵乘法的两种理解
- 空间变化，所有的坐标都是基于一组特定的基，无论是原始向量，以及变换矩阵和变换后的向量。
- 坐标变化，a(a1, a2)基座标下的向量 r_a 变换到b(b1, b2)坐标下的向量，则将b1, b2由a(a1, a2表示)，形成基变化矩阵P，乘以 r_a, 将r_a变化为r_b。


